{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04876200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from movinets import MoViNet\n",
    "from movinets.config import _C\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "177a281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def prepare_dataset_paths_and_labels_recursive(root_dir, class_map):\n",
    "    video_folders = []\n",
    "    labels = []\n",
    "    for class_name, label_id in class_map.items():\n",
    "        class_path = os.path.join(root_dir, class_name)\n",
    "        if not os.path.exists(class_path):\n",
    "            continue\n",
    "        # Walk recursively starting from class_path\n",
    "        for subdir, dirs, files in os.walk(class_path):\n",
    "            # If this folder contains image frames, treat it as a video folder\n",
    "            if any(f.lower().endswith(('.jpg', '.png')) for f in files):\n",
    "                video_folders.append(subdir)\n",
    "                labels.append(label_id)\n",
    "    return video_folders, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed5e4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_class_map = {\n",
    "    'Tiger_Normal': 0,\n",
    "    'Tiger_Abnormal': 1,\n",
    "}\n",
    "\n",
    "multi_class_map = {\n",
    "    'Dehydration or Heat Stroke': 0,\n",
    "    'Digestive Issues': 1,\n",
    "    'Eye Injury': 2,\n",
    "    'Injured_Tiger': 3,\n",
    "    'Lethargy, Apathy, Unresponsive, and Listless Tiger': 4,\n",
    "    'Neurological Issues': 5,\n",
    "    'Nutritional_Deficiencies': 6,\n",
    "    'Oral or Dental Issues or Respiratory distress': 7,\n",
    "    'Skin Desease or irritation_Tiger': 8,\n",
    "    'Sress_Frustation': 9,\n",
    "    'Tremors or Seizures': 10,\n",
    "    'underweightness or emaciation': 11,\n",
    "    'Weakness': 12,\n",
    "    'Zoochosis_stereotypic behavior': 13,\n",
    "    'Zoonotic Disease Behavior': 14\n",
    "\n",
    "}\n",
    "\n",
    "multi_class_id_to_name = {v: k for k, v in multi_class_map.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1b221b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = 'frames_dataset/train'\n",
    "test_root = 'frames_dataset/test'\n",
    "\n",
    "train_video_folders_bin, train_labels_bin = prepare_dataset_paths_and_labels_recursive(train_root, binary_class_map)\n",
    "test_video_folders_bin, test_labels_bin = prepare_dataset_paths_and_labels_recursive(test_root, binary_class_map)\n",
    "\n",
    "train_abnormal_root = os.path.join(train_root, 'Tiger_Abnormal')\n",
    "test_abnormal_root = os.path.join(test_root, 'Tiger_Abnormal')\n",
    "train_video_folders_multi, train_labels_multi = prepare_dataset_paths_and_labels_recursive(train_abnormal_root, multi_class_map)\n",
    "test_video_folders_multi, test_labels_multi = prepare_dataset_paths_and_labels_recursive(test_abnormal_root, multi_class_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c00d414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_3d_pose(frames):\n",
    "    \"\"\"\n",
    "    Dummy example function: in reality, load and run your 3D pose estimator model here.\n",
    "    Input: frames as list of RGB numpy arrays or tensor of shape [clip_len, C, H, W]\n",
    "    Output: 3D keypoints tensor of shape [clip_len, num_keypoints * 3] (x, y, z coordinates)\n",
    "    \"\"\"\n",
    "    clip_len = len(frames)                                                                              \n",
    "    \n",
    "    num_keypoints = 34\n",
    "    # Dummy random 3D keypoints to illustrate; replace with your model inference\n",
    "    pose_3d = torch.rand(clip_len, num_keypoints * 3)\n",
    "    return pose_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d068c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class TigerBehaviorDataset(Dataset):\n",
    "    def __init__(self, video_folders, labels, pose_folder_root=None,\n",
    "                 clip_len=8, frame_size=(224, 224), transform=None,\n",
    "                 use_3d_pose=False):\n",
    "        self.video_folders = video_folders\n",
    "        self.labels = labels\n",
    "        self.pose_folder_root = pose_folder_root\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_size = frame_size\n",
    "        self.transform = transform\n",
    "        self.use_3d_pose = use_3d_pose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_folder = self.video_folders[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        frame_files = sorted([\n",
    "            os.path.join(video_folder, f)\n",
    "            for f in os.listdir(video_folder)\n",
    "            if f.lower().endswith(('.jpg','.png'))\n",
    "        ])\n",
    "        if len(frame_files) == 0:\n",
    "            raise RuntimeError(f\"No frames found in folder {video_folder}\")\n",
    "\n",
    "        if len(frame_files) < self.clip_len:\n",
    "            frame_files += [frame_files[-1]] * (self.clip_len - len(frame_files))\n",
    "        else:\n",
    "            frame_files = frame_files[:self.clip_len]\n",
    "\n",
    "        frames = []\n",
    "        for fpath in frame_files:\n",
    "            img = cv2.imread(fpath)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, self.frame_size)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            else:\n",
    "                img = torch.FloatTensor(img / 255.0).permute(2, 0, 1)\n",
    "            frames.append(img)\n",
    "        frames_tensor = torch.stack(frames, dim=1)  # Shape: (C, T, H, W)\n",
    "\n",
    "        if self.use_3d_pose:\n",
    "            # Estimate 3D pose dynamically\n",
    "            # Convert frames to list or tensor shape suitable for your estimator\n",
    "            frames_for_pose = [frame.permute(1, 2, 0).numpy() for frame in frames]  # list of HWC images\n",
    "            pose_3d_tensor = estimate_3d_pose(frames_for_pose)  # shape: [clip_len, num_keypoints*3]\n",
    "        elif self.pose_folder_root:\n",
    "            # Load precomputed 2D/3D poses from file\n",
    "            pose_filename = os.path.basename(video_folder) + '.npy'\n",
    "            pose_path = os.path.join(self.pose_folder_root, pose_filename)\n",
    "            pose_seq = np.load(pose_path)\n",
    "            pose_3d_tensor = torch.from_numpy(pose_seq).float()\n",
    "        else:\n",
    "            # If no poses available, provide zeros (adjust input dimension accordingly)\n",
    "            pose_3d_tensor = torch.zeros(self.clip_len, 34 * 3)  # assuming 3D with 34 keypoints\n",
    "\n",
    "        return frames_tensor, pose_3d_tensor, label\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dad21ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Binary datasets and loaders\n",
    "train_dataset_bin = TigerBehaviorDataset(train_video_folders_bin, train_labels_bin, pose_folder_root=None, clip_len=8)\n",
    "test_dataset_bin = TigerBehaviorDataset(test_video_folders_bin, test_labels_bin, pose_folder_root=None, clip_len=8)\n",
    "\n",
    "import multiprocessing as mp\n",
    "train_loader_bin = DataLoader(\n",
    "    train_dataset_bin,\n",
    "    batch_size=32,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader_bin = DataLoader(test_dataset_bin, batch_size=6, shuffle=False, num_workers=0)\n",
    "\n",
    "# Multi-class datasets and loaders (only abnormal)\n",
    "train_dataset_multi = TigerBehaviorDataset(train_video_folders_multi, train_labels_multi, pose_folder_root=None, clip_len=8)\n",
    "test_dataset_multi = TigerBehaviorDataset(test_video_folders_multi, test_labels_multi, pose_folder_root=None, clip_len=8)\n",
    "train_loader_multi = DataLoader(train_dataset_multi, batch_size=8, shuffle=True, num_workers=0)\n",
    "test_loader_multi = DataLoader(test_dataset_multi, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a377a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class DummyMoViNet(nn.Module):\n",
    "    def __init__(self, feat_dim=600):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "    def forward(self, x):\n",
    "        # Dummy output: random tensor for demo\n",
    "        return torch.randn(x.size(0), self.feat_dim).to(x.device)\n",
    "\n",
    "class PoseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=34, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=2)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch, input_dim)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)\n",
    "        return self.fc(x)\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, movinet_feat_dim=600, pose_feat_dim=34, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(movinet_feat_dim + pose_feat_dim, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, movinet_feat, pose_feat):\n",
    "        x = torch.cat([movinet_feat, pose_feat], dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0a8c282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FusionClassifier(\n",
       "  (fc1): Linear(in_features=602, out_features=512, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "movinet_bin = DummyMoViNet()\n",
    "pose_model_bin = PoseTransformer(input_dim=102, num_classes=2).to(device)\n",
    "fusion_model_bin = FusionClassifier(movinet_feat_dim=600, pose_feat_dim=2, num_classes=2).to(device)\n",
    "\n",
    "movinet_bin.to(device)\n",
    "pose_model_bin.to(device)\n",
    "fusion_model_bin.to(device)\n",
    "\n",
    "# Multi-class models\n",
    "movinet_multi = DummyMoViNet()\n",
    "pose_model_multi = PoseTransformer(input_dim=102, num_classes=len(multi_class_map)).to(device)\n",
    "fusion_model_multi = FusionClassifier(movinet_feat_dim=600, pose_feat_dim=2, num_classes=len(multi_class_map)).to(device)\n",
    "\n",
    "movinet_multi.to(device)\n",
    "pose_model_multi.to(device)\n",
    "fusion_model_multi.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9af2dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_video(frames_tensor, pose_tensor,\n",
    "                movinet_bin, pose_model_bin, fusion_model_bin,\n",
    "                movinet_multi, pose_model_multi, fusion_model_multi,\n",
    "                multi_class_id_to_name, device):\n",
    "\n",
    "    frames_tensor = frames_tensor.unsqueeze(0).to(device)  # Add batch dim\n",
    "    pose_tensor = pose_tensor.unsqueeze(0).to(device)      # Add batch dim\n",
    "    pose_input = pose_tensor.permute(1,0,2)  # (seq_len, batch, pose_dim)\n",
    "\n",
    "    movinet_bin.eval()\n",
    "    pose_model_bin.eval()\n",
    "    fusion_model_bin.eval()\n",
    "\n",
    "    movinet_multi.eval()\n",
    "    pose_model_multi.eval()\n",
    "    fusion_model_multi.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Binary prediction\n",
    "        movinet_feats_bin = movinet_bin(frames_tensor)\n",
    "        pose_feats_bin = pose_model_bin(pose_input)\n",
    "        logits_bin = fusion_model_bin(movinet_feats_bin, pose_feats_bin)\n",
    "        pred_bin = torch.argmax(logits_bin, dim=1).item()\n",
    "\n",
    "        if pred_bin == 0:\n",
    "            # Normal class\n",
    "            return \"Normal\"\n",
    "        else:\n",
    "            # Abnormal class → run multi-class inference\n",
    "            movinet_feats_multi = movinet_multi(frames_tensor)\n",
    "            pose_feats_multi = pose_model_multi(pose_input)\n",
    "            logits_multi = fusion_model_multi(movinet_feats_multi, pose_feats_multi)\n",
    "            pred_multi = torch.argmax(logits_multi, dim=1).item()\n",
    "            subclass_name = multi_class_id_to_name.get(pred_multi, \"Unknown Abnormal Class\")\n",
    "            return f\"Abnormal - Subclass: {subclass_name}\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c249522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference result: Normal\n"
     ]
    }
   ],
   "source": [
    "# Example: load one sample from your dataset (binary or multi-class does not matter)\n",
    "frames_tensor, pose_tensor, label = test_dataset_bin[0]\n",
    "\n",
    "# Run inference\n",
    "result = infer_video(frames_tensor, pose_tensor,\n",
    "                     movinet_bin, pose_model_bin, fusion_model_bin,\n",
    "                     movinet_multi, pose_model_multi, fusion_model_multi,\n",
    "                     multi_class_id_to_name,\n",
    "                     device)\n",
    "\n",
    "print(\"Inference result:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b17151d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume frames_tensor and pose_tensor are already loaded and prepared inputs\n",
    "movinet_bin.eval()\n",
    "pose_model_bin.eval()\n",
    "fusion_model_bin.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    movinet_feats_bin = movinet_bin(frames_tensor.unsqueeze(0).to(device))  # batch dimension added\n",
    "    pose_feats_bin = pose_model_bin(pose_tensor.unsqueeze(0).permute(1, 0, 2).to(device))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ffa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoViNet feature shape: torch.Size([1, 600])\n",
      "Pose feature shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"MoViNet feature shape:\", movinet_feats_bin.shape)\n",
    "print(\"Pose feature shape:\", pose_feats_bin.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efb984dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames shape: torch.Size([32, 3, 8, 224, 224])\n",
      "poses shape before permute: torch.Size([32, 8, 102])\n",
      "poses shape after permute: torch.Size([8, 32, 102])\n",
      "labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for frames, poses, labels in train_loader_bin:\n",
    "    print(\"frames shape:\", frames.shape)       # [batch_size, 3, seq_len, 224, 224]\n",
    "    print(\"poses shape before permute:\", poses.shape)  # [batch_size, seq_len, pose_dim]\n",
    "    poses = poses.permute(1, 0, 2)  # Permute if needed\n",
    "    print(\"poses shape after permute:\", poses.shape)  \n",
    "    print(\"labels shape:\", labels.shape)\n",
    "\n",
    "    # Break early if you want to just check one batch\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aff69a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 3/3 [00:19<00:00,  6.52s/batch, accuracy=0.3830, loss=0.7162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train loss: 0.7162, Train accuracy: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.37s/batch, accuracy=0.3191, loss=0.7262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Train loss: 0.7262, Train accuracy: 0.3191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 3/3 [00:09<00:00,  3.32s/batch, accuracy=0.3830, loss=0.7156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Train loss: 0.7156, Train accuracy: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.58s/batch, accuracy=0.2447, loss=0.7378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Train loss: 0.7378, Train accuracy: 0.2447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 3/3 [00:09<00:00,  3.31s/batch, accuracy=0.3830, loss=0.7208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - Train loss: 0.7208, Train accuracy: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 3/3 [00:09<00:00,  3.31s/batch, accuracy=0.3085, loss=0.7237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - Train loss: 0.7237, Train accuracy: 0.3085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 3/3 [00:09<00:00,  3.30s/batch, accuracy=0.2447, loss=0.7323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - Train loss: 0.7323, Train accuracy: 0.2447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.37s/batch, accuracy=0.3191, loss=0.7254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - Train loss: 0.7254, Train accuracy: 0.3191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|██████████| 3/3 [00:09<00:00,  3.16s/batch, accuracy=0.2766, loss=0.7243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - Train loss: 0.7243, Train accuracy: 0.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████| 3/3 [00:09<00:00,  3.18s/batch, accuracy=0.2766, loss=0.7258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - Train loss: 0.7258, Train accuracy: 0.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████| 3/3 [00:09<00:00,  3.24s/batch, accuracy=0.2766, loss=0.7295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Train loss: 0.7295, Train accuracy: 0.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.39s/batch, accuracy=0.3617, loss=0.7194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - Train loss: 0.7194, Train accuracy: 0.3617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.40s/batch, accuracy=0.2979, loss=0.7267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - Train loss: 0.7267, Train accuracy: 0.2979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.39s/batch, accuracy=0.3617, loss=0.7205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - Train loss: 0.7205, Train accuracy: 0.3617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.43s/batch, accuracy=0.3298, loss=0.7170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - Train loss: 0.7170, Train accuracy: 0.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████| 3/3 [00:09<00:00,  3.30s/batch, accuracy=0.2872, loss=0.7237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - Train loss: 0.7237, Train accuracy: 0.2872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.42s/batch, accuracy=0.3298, loss=0.7198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - Train loss: 0.7198, Train accuracy: 0.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.41s/batch, accuracy=0.2979, loss=0.7191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - Train loss: 0.7191, Train accuracy: 0.2979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.60s/batch, accuracy=0.4362, loss=0.7100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - Train loss: 0.7100, Train accuracy: 0.4362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.58s/batch, accuracy=0.3298, loss=0.7231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - Train loss: 0.7231, Train accuracy: 0.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.51s/batch, accuracy=0.3191, loss=0.7166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Train loss: 0.7166, Train accuracy: 0.3191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.51s/batch, accuracy=0.2766, loss=0.7261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - Train loss: 0.7261, Train accuracy: 0.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.39s/batch, accuracy=0.3511, loss=0.7232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - Train loss: 0.7232, Train accuracy: 0.3511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.39s/batch, accuracy=0.2553, loss=0.7265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - Train loss: 0.7265, Train accuracy: 0.2553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.38s/batch, accuracy=0.3617, loss=0.7162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - Train loss: 0.7162, Train accuracy: 0.3617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.38s/batch, accuracy=0.3511, loss=0.7292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - Train loss: 0.7292, Train accuracy: 0.3511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.42s/batch, accuracy=0.3936, loss=0.7158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - Train loss: 0.7158, Train accuracy: 0.3936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.41s/batch, accuracy=0.3830, loss=0.7168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - Train loss: 0.7168, Train accuracy: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.41s/batch, accuracy=0.3830, loss=0.7108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - Train loss: 0.7108, Train accuracy: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|██████████| 3/3 [00:10<00:00,  3.66s/batch, accuracy=0.3298, loss=0.7168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - Train loss: 0.7168, Train accuracy: 0.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "pose_model_bin = pose_model_bin.to(device)\n",
    "fusion_model_bin = fusion_model_bin.to(device)\n",
    "movinet_bin = movinet_bin.to(device)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    list(pose_model_bin.parameters()) +\n",
    "    list(fusion_model_bin.parameters()) +\n",
    "    list(movinet_bin.parameters()),\n",
    "    lr=0.02\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_acc_metric = Accuracy(task=\"binary\").to(device)\n",
    "val_acc_metric = Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pose_model_bin.train()\n",
    "    fusion_model_bin.train()\n",
    "    train_acc_metric.reset()\n",
    "    running_train_loss = 0.0\n",
    "    train_samples = 0\n",
    "\n",
    "    with tqdm(train_loader_bin, unit=\"batch\") as train_iter:\n",
    "        train_iter.set_description(f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "\n",
    "        for frames, poses, labels in train_iter:\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.to(device)\n",
    "            poses = poses.permute(1, 0, 2).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                movinet_feats = movinet_bin(frames)\n",
    "\n",
    "            pose_feats = pose_model_bin(poses)\n",
    "            outputs = fusion_model_bin(movinet_feats, pose_feats)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_acc_metric.update(preds, labels)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            running_train_loss += loss.item() * batch_size\n",
    "            train_samples += batch_size\n",
    "\n",
    "            train_acc = train_acc_metric.compute().item()\n",
    "            train_iter.set_postfix(loss=f\"{running_train_loss/train_samples:.4f}\", accuracy=f\"{train_acc:.4f}\")\n",
    "\n",
    "    epoch_train_loss = running_train_loss / train_samples\n",
    "    epoch_train_acc = train_acc_metric.compute().item()\n",
    "\n",
    "    # Validation loop needs val_loader_bin similarly defined and active, else skip\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "        f\"Train loss: {epoch_train_loss:.4f}, Train accuracy: {epoch_train_acc:.4f}\"\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf80bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
